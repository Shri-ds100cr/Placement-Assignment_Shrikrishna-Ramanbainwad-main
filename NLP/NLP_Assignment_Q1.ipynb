{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMDf7pWm1qkmWlKsv88zqfM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Q-1. Take any YouTube videos link and your task is to extract the comments from that videos and store it in a csv file and then you need define what is most demanding topic in that videos comment section\n","\n"],"metadata":{"id":"gYp4Ytxxf_XQ"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"_OdRosCbf0sg","executionInfo":{"status":"ok","timestamp":1686034775668,"user_tz":-330,"elapsed":2360,"user":{"displayName":"Rushikesh Khandare","userId":"05904911094346435490"}}},"outputs":[],"source":["import requests\n","import pandas as pd\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from gensim import models\n","from gensim.corpora import Dictionary"]},{"cell_type":"code","source":["# Download NLTK data\n","nltk.download('punkt')\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KTS3j129gKNm","executionInfo":{"status":"ok","timestamp":1686035922520,"user_tz":-330,"elapsed":551,"user":{"displayName":"Rushikesh Khandare","userId":"05904911094346435490"}},"outputId":"69ab7ae7-195f-4b0c-cd5a-f6146699f659"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# Function to fetch comments from a YouTube video using the YouTube Data API\n","def fetch_youtube_comments(video_id, api_key):\n","    base_url = 'https://www.googleapis.com/youtube/v3/commentThreads'\n","    params = {\n","        'part': 'snippet',\n","        'videoId': video_id,\n","        'maxResults': 100,\n","        'key': api_key\n","    }\n","    comments = []\n","    next_page_token = None\n","    while True:\n","        if next_page_token:\n","            params['pageToken'] = next_page_token\n","        response = requests.get(base_url, params=params)\n","        data = response.json()\n","        for item in data['items']:\n","            comment = item['snippet']['topLevelComment']['snippet']['textOriginal']\n","            comments.append(comment)\n","        if 'nextPageToken' in data:\n","            next_page_token = data['nextPageToken']\n","        else:\n","            break\n","    return comments"],"metadata":{"id":"HtHIHJ_QgTjT","executionInfo":{"status":"ok","timestamp":1686035927299,"user_tz":-330,"elapsed":512,"user":{"displayName":"Rushikesh Khandare","userId":"05904911094346435490"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Function to preprocess the comments\n","def preprocess_comments(comments):\n","    processed_comments = []\n","    for comment in comments:\n","        # Convert to lowercase\n","        comment = comment.lower()\n","        # Remove special characters, URLs, and non-alphanumeric characters\n","        comment = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", comment)\n","        comment = re.sub(r\"http\\S+|www\\S+\", \"\", comment)\n","        # Tokenize comment into words\n","        words = word_tokenize(comment)\n","        # Remove stop words and perform stemming\n","        stop_words = set(stopwords.words('english'))\n","        stemmer = PorterStemmer()\n","        words = [stemmer.stem(word) for word in words if word not in stop_words]\n","        # Add processed comment if it contains terms\n","        if len(words) > 0:\n","            processed_comments.append(words)\n","    return processed_comments"],"metadata":{"id":"w5khLQ2agk4B","executionInfo":{"status":"ok","timestamp":1686035931411,"user_tz":-330,"elapsed":696,"user":{"displayName":"Rushikesh Khandare","userId":"05904911094346435490"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Function to calculate the most demanding topic based on comment count\n","def find_demanding_topic(comments):\n","    # Create a dictionary from the comments\n","    dictionary = Dictionary(comments)\n","    # Create a corpus (bag of words) representation\n","    corpus = [dictionary.doc2bow(comment) for comment in comments]\n","    # Perform LDA topic modeling\n","    lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=10)\n","    # Calculate comment count for each topic\n","    topic_comment_counts = {topic: 0 for topic in range(lda_model.num_topics)}\n","    for comment in comments:\n","        topic = lda_model.get_document_topics(dictionary.doc2bow(comment), minimum_probability=0.2)\n","        dominant_topic = max(topic, key=lambda x: x[1])[0]\n","        topic_comment_counts[dominant_topic] += 1\n","    # Find the most demanding topic based on comment count\n","    most_demanding_topic = max(topic_comment_counts, key=topic_comment_counts.get)\n","    return most_demanding_topic"],"metadata":{"id":"tCjfldetgt6_","executionInfo":{"status":"ok","timestamp":1686035933000,"user_tz":-330,"elapsed":3,"user":{"displayName":"Rushikesh Khandare","userId":"05904911094346435490"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Function to save comments to a CSV file\n","def save_comments_to_csv(comments, csv_file):\n","    df = pd.DataFrame({'Comment': comments})\n","    df.to_csv(csv_file, index=False)\n","    print('Comments saved to', csv_file)\n","# Specify the YouTube video ID and API key\n","video_id = 'l37XiBGV3fE'\n","api_key = 'AIzaSyD6OKEneD3U__Qzp3uFVk8X05DtkIP2Vww'\n","# Fetch comments from the YouTube video\n","comments = fetch_youtube_comments(video_id, api_key)\n","# Check if comments are available\n","if len(comments) > 0:\n","    # Specify the CSV file path for comments\n","    comments_csv_file = 'comments.csv'\n","    # Save comments to a CSV file\n","    save_comments_to_csv(comments, comments_csv_file)\n","    # Preprocess the comments\n","    preprocessed_comments = preprocess_comments(comments)\n","    # Perform demanding topic analysis if preprocessed comments are available\n","    if len(preprocessed_comments) > 0:\n","        # Find the most demanding topic based on comment count\n","        most_demanding_topic = find_demanding_topic(preprocessed_comments)\n","        print(\"Most Demanding Topic (based on comment count):\", most_demanding_topic)\n","else:\n","    print(\"No comments available.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jiTqJ11Kg05X","executionInfo":{"status":"ok","timestamp":1686035952955,"user_tz":-330,"elapsed":17958,"user":{"displayName":"Rushikesh Khandare","userId":"05904911094346435490"}},"outputId":"ef65e5a2-72c7-431b-c91f-ac80f5f4f9a5"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Comments saved to comments.csv\n","Most Demanding Topic (based on comment count): 1\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"AMzrCJz7kqrd"},"execution_count":null,"outputs":[]}]}