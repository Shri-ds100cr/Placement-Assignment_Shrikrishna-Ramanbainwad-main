{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1 -\n",
        "# Implement 3 different CNN architectures with a comparison table for the MNSIT\n",
        "# dataset using the Tensorflow library.\n",
        "# Note -\n",
        "# 1. The model parameters for each architecture should not be more than 8000\n",
        "# parameters\n",
        "# 2. Code comments should be given for proper code understanding.\n",
        "# 3. The minimum accuracy for each accuracy should be at least 96%"
      ],
      "metadata": {
        "id": "jdGnzzu9aoHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
        "\n",
        "# Architecture 1: Simple CNN\n",
        "model_1 = tf.keras.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile and train the model\n",
        "model_1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_1.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
        "\n",
        "# Calculate accuracy and parameters\n",
        "accuracy_1 = model_1.evaluate(x_test, y_test)[1]\n",
        "parameters_1 = model_1.count_params()\n",
        "\n",
        "print(\"Architecture 1:\")\n",
        "print(\"Accuracy:\", accuracy_1)\n",
        "print(\"Parameters:\", parameters_1)\n",
        "\n",
        "# Architecture 2: Modified CNN\n",
        "model_2 = tf.keras.Sequential([\n",
        "    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile and train the model\n",
        "model_2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_2.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
        "\n",
        "# Calculate accuracy and parameters\n",
        "accuracy_2 = model_2.evaluate(x_test, y_test)[1]\n",
        "parameters_2 = model_2.count_params()\n",
        "\n",
        "print(\"Architecture 2:\")\n",
        "print(\"Accuracy:\", accuracy_2)\n",
        "print(\"Parameters:\", parameters_2)\n",
        "\n",
        "# Architecture 3: Deep CNN\n",
        "model_3 = tf.keras.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile and train the model\n",
        "model_3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_3.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
        "\n",
        "# Calculate accuracy and parameters\n",
        "accuracy_3 = model_3.evaluate(x_test, y_test)[1]\n",
        "parameters_3 = model_3.count_params()\n",
        "\n",
        "print(\"Architecture 3:\")\n",
        "print(\"Accuracy:\", accuracy_3)\n",
        "print(\"Parameters:\", parameters_3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UdoTteAbgDI",
        "outputId": "bd73e153-ba95-475a-b1bf-d8313cc7ab6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.1783 - accuracy: 0.9477 - val_loss: 0.0661 - val_accuracy: 0.9792\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0629 - accuracy: 0.9811 - val_loss: 0.0590 - val_accuracy: 0.9804\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0428 - accuracy: 0.9865 - val_loss: 0.0516 - val_accuracy: 0.9824\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0297 - accuracy: 0.9908 - val_loss: 0.0511 - val_accuracy: 0.9844\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0220 - accuracy: 0.9930 - val_loss: 0.0500 - val_accuracy: 0.9839\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0500 - accuracy: 0.9839\n",
            "Architecture 1:\n",
            "Accuracy: 0.9839000105857849\n",
            "Parameters: 347146\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 9s 4ms/step - loss: 0.1509 - accuracy: 0.9539 - val_loss: 0.0540 - val_accuracy: 0.9832\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0492 - accuracy: 0.9850 - val_loss: 0.0360 - val_accuracy: 0.9886\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0346 - accuracy: 0.9888 - val_loss: 0.0333 - val_accuracy: 0.9892\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0256 - accuracy: 0.9919 - val_loss: 0.0389 - val_accuracy: 0.9869\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0195 - accuracy: 0.9937 - val_loss: 0.0346 - val_accuracy: 0.9896\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0346 - accuracy: 0.9896\n",
            "Architecture 2:\n",
            "Accuracy: 0.9896000027656555\n",
            "Parameters: 108618\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 9s 4ms/step - loss: 0.0989 - accuracy: 0.9697 - val_loss: 0.0363 - val_accuracy: 0.9876\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0342 - accuracy: 0.9897 - val_loss: 0.0436 - val_accuracy: 0.9849\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0226 - accuracy: 0.9930 - val_loss: 0.0289 - val_accuracy: 0.9907\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0158 - accuracy: 0.9950 - val_loss: 0.0305 - val_accuracy: 0.9909\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0131 - accuracy: 0.9958 - val_loss: 0.0315 - val_accuracy: 0.9914\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0315 - accuracy: 0.9914\n",
            "Architecture 3:\n",
            "Accuracy: 0.9914000034332275\n",
            "Parameters: 876362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2 -\n",
        "\n",
        "# Implement 5 different CNN architectures with a comparison table for CIFAR 10\n",
        "\n",
        "# dataset using the PyTorch library\n",
        "\n",
        "# Note -\n",
        "\n",
        "# 1. The model parameters for each architecture should not be more than 10000 parameters\n",
        "\n",
        "# 2 Code comments should be given for proper code understanding"
      ],
      "metadata": {
        "id": "5tFXZMC0dhOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the transforms for data preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images to range [-1, 1]\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define the data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Define the CNN architectures\n",
        "class Model1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model1, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(16 * 16 * 16, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = x.view(-1, 16 * 16 * 16)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "class Model2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32 * 16 * 16, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = x.view(-1, 32 * 16 * 16)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "class Model3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model3, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 16 * 16, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = x.view(-1, 64 * 16 * 16)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "class Model4(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model4, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "class Model5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 128 * 8 * 8)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the models\n",
        "model1 = Model1()\n",
        "model2 = Model2()\n",
        "model3 = Model3()\n",
        "model4 = Model4()\n",
        "model5 = Model5()\n",
        "\n",
        "# Function to count the number of parameters in a model\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Calculate the number of parameters for each model\n",
        "params1 = count_parameters(model1)\n",
        "params2 = count_parameters(model2)\n",
        "params3 = count_parameters(model3)\n",
        "params4 = count_parameters(model4)\n",
        "params5 = count_parameters(model5)\n",
        "\n",
        "# Comparison table\n",
        "print(\"Model\\t\\t\\tParameters\")\n",
        "print(\"------------------------------------------------\")\n",
        "print(\"Model 1:\\t\\t{}\".format(params1))\n",
        "print(\"Model 2:\\t\\t{}\".format(params2))\n",
        "print(\"Model 3:\\t\\t{}\".format(params3))\n",
        "print(\"Model 4:\\t\\t{}\".format(params4))\n",
        "print(\"Model 5:\\t\\t{}\".format(params5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkJNlwGXctEJ",
        "outputId": "8d846095-f7d4-4d4b-f662-2386846d8769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 49027452.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Model\t\t\tParameters\n",
            "------------------------------------------------\n",
            "Model 1:\t\t41418\n",
            "Model 2:\t\t82826\n",
            "Model 3:\t\t165642\n",
            "Model 4:\t\t60362\n",
            "Model 5:\t\t157578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Question 3 -\n",
        "\n",
        "Train a Pure CNN with less than 10000 trainable parameters using the MNIST\n",
        "\n",
        "Dataset having minimum validation accuracy of 99.40%\n",
        "\n",
        "Note -\n",
        "\n",
        "1. Code comments should be given for proper code understanding.\n",
        "\n",
        "2. Implement in both PyTorch and Tensorflow respectively"
      ],
      "metadata": {
        "id": "qxmbJaGEfPPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the transforms for data preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize images to mean 0.1307 and std 0.3081\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define the data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the Pure CNN model\n",
        "class PureCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PureCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc = nn.Linear(32 * 7 * 7, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "model = PureCNN()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print loss after each epoch\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "# Evaluation on the test set\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXGpN-gOgtWo",
        "outputId": "8e302b28-c962-4a1e-d317-b82484c8bc0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.17893448707186924\n",
            "Epoch [2/10], Loss: 0.05545071037540804\n",
            "Epoch [3/10], Loss: 0.040522300890698505\n",
            "Epoch [4/10], Loss: 0.033625753176188086\n",
            "Epoch [5/10], Loss: 0.026825589423484505\n",
            "Epoch [6/10], Loss: 0.022461375310984893\n",
            "Epoch [7/10], Loss: 0.01816290431037999\n",
            "Epoch [8/10], Loss: 0.016273609294336345\n",
            "Epoch [9/10], Loss: 0.012941731330026272\n",
            "Epoch [10/10], Loss: 0.012178188239694384\n",
            "Test Accuracy: 98.91%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
        "\n",
        "# Define the Pure CNN model\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=64)\n",
        "\n",
        "# Evaluation on the test set\n",
        "_, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJhxIM7Hf55q",
        "outputId": "6684d8d1-e9aa-4190-8411-652b46ad44d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "938/938 [==============================] - 5s 4ms/step - loss: 0.1755 - accuracy: 0.9492\n",
            "Epoch 2/10\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 0.0549 - accuracy: 0.9833\n",
            "Epoch 3/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0372 - accuracy: 0.9884\n",
            "Epoch 4/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0274 - accuracy: 0.9914\n",
            "Epoch 5/10\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 0.0193 - accuracy: 0.9938\n",
            "Epoch 6/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0142 - accuracy: 0.9952\n",
            "Epoch 7/10\n",
            "938/938 [==============================] - 3s 4ms/step - loss: 0.0105 - accuracy: 0.9966\n",
            "Epoch 8/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0081 - accuracy: 0.9976\n",
            "Epoch 9/10\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 0.0063 - accuracy: 0.9978\n",
            "Epoch 10/10\n",
            "938/938 [==============================] - 3s 4ms/step - loss: 0.0063 - accuracy: 0.9979\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0539 - accuracy: 0.9883\n",
            "Test Accuracy: 98.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YfPWaMC9f6hD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}